{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.1)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2022.7)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 122.9/250.9 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  245.8/250.9 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 250.9/250.9 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n\\ndata = {\\n    \"Website_URLs\": [\\n        \"https://www.example.com\",\\n        \"https://www.wikipedia.org\",\\n        \"https://www.python.org\"\\n    ]\\n}\\n\\nexcel_file_path = \"website_urls.xlsx\"\\ndf = pd.DataFrame(data)\\ndf.to_excel(excel_file_path, index=False)\\nprint(f\"Excel file with URLs created at: {excel_file_path}\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is to create an excel file.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Website_URLs\": [\n",
    "        \"https://www.example.com\",\n",
    "        \"https://www.wikipedia.org\",\n",
    "        \"https://www.python.org\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "excel_file_path = \"website_urls.xlsx\"\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "print(f\"Excel file with URLs created at: {excel_file_path}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved at: cleaned_website_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch and clean data\n",
    "def fetch_and_clean_data(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        cleaned_text = soup.get_text(separator=\" \", strip=True)\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching URL {url}: {e}\"\n",
    "\n",
    "# Read URLs from the Excel file\n",
    "input_file = \"website_urls.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Process each URL\n",
    "cleaned_data = []\n",
    "for url in df[\"Website_URLs\"]:\n",
    "    cleaned_data.append(fetch_and_clean_data(url))\n",
    "\n",
    "# Save the cleaned data to a new Excel file\n",
    "df[\"Cleaned_Text\"] = cleaned_data\n",
    "output_file = \"cleaned_website_data.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "print(f\"Cleaned data saved at: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"cleaned_website_data.xlsx\"  # Replace with your actual file path\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(raw_text):\n",
    "    try:\n",
    "        # Step 1: Remove HTML tags and scripts\n",
    "        soup = BeautifulSoup(raw_text, \"html.parser\")\n",
    "        cleaned_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        # Step 2: Normalize whitespace\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "        # Step 3: Remove special characters\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned_text)\n",
    "\n",
    "        # Step 4: Convert to lowercase\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "\n",
    "        # Step 5 (Optional): Tokenization (if needed, uncomment below)\n",
    "        # tokens = cleaned_text.split()\n",
    "\n",
    "        # Step 6 (Optional): Remove stopwords (if needed, uncomment below)\n",
    "        # from nltk.corpus import stopwords\n",
    "        # stop_words = set(stopwords.words('english'))\n",
    "        # tokens = [word for word in tokens if word not in stop_words]\n",
    "        # cleaned_text = \" \".join(tokens)\n",
    "\n",
    "        return cleaned_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error processing text: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to: processed_website_data.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C-Dac\\AppData\\Local\\Temp\\ipykernel_10632\\663602529.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(raw_text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the \"Cleaned_Text\" column\n",
    "processed_df = df[['Cleaned_Text']].copy()\n",
    "processed_df['Processed_Text'] = processed_df['Cleaned_Text'].apply(preprocess_text)\n",
    "\n",
    "# Save the processed data to a separate Excel file\n",
    "output_file = \"processed_website_data.xlsx\"\n",
    "processed_df.to_excel(output_file, index=False)\n",
    "print(f\"Processed data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load processed data\n",
    "file_path = \"processed_website_data.xlsx\"\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C-Dac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "df['Embeddings'] = df['Processed_Text'].apply(lambda text: model.encode(text).tolist())\n",
    "\n",
    "# Save embeddings\n",
    "df.to_pickle(\"processed_data_with_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "df = pd.read_pickle(\"processed_data_with_embeddings.pkl\")\n",
    "embeddings = np.array(df['Embeddings'].tolist(), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance metric\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"semantic_search_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate query embedding\n",
    "query = \"Latest cricket news\"\n",
    "query_embedding = model.encode(query).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Processed_Text\n",
      "21  todays cricket match  cricket update  cricket ...\n",
      "0   live cricket score schedule latest news stats ...\n",
      "15  sporting news india  nba  cricket  football  t...\n",
      "10  bbc sport  scores fixtures news  live sport bb...\n",
      "8   the athletic uk  sports news commentary result...\n"
     ]
    }
   ],
   "source": [
    "# Search the index\n",
    "D, I = index.search(query_embedding, k=5)  # Retrieve top 5 results\n",
    "results = df.iloc[I[0]]\n",
    "print(results[['Processed_Text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
