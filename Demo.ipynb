{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.1)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2022.7)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\c-dac\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 122.9/250.9 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  245.8/250.9 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 250.9/250.9 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n\\ndata = {\\n    \"Website_URLs\": [\\n        \"https://www.example.com\",\\n        \"https://www.wikipedia.org\",\\n        \"https://www.python.org\"\\n    ]\\n}\\n\\nexcel_file_path = \"website_urls.xlsx\"\\ndf = pd.DataFrame(data)\\ndf.to_excel(excel_file_path, index=False)\\nprint(f\"Excel file with URLs created at: {excel_file_path}\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is to create an excel file.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Website_URLs\": [\n",
    "        \"https://www.example.com\",\n",
    "        \"https://www.wikipedia.org\",\n",
    "        \"https://www.python.org\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "excel_file_path = \"website_urls.xlsx\"\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "print(f\"Excel file with URLs created at: {excel_file_path}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved at: cleaned_website_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch and clean data\n",
    "def fetch_and_clean_data(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        cleaned_text = soup.get_text(separator=\" \", strip=True)\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching URL {url}: {e}\"\n",
    "\n",
    "# Read URLs from the Excel file\n",
    "input_file = \"website_urls.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Process each URL\n",
    "cleaned_data = []\n",
    "for url in df[\"Website_URLs\"]:\n",
    "    cleaned_data.append(fetch_and_clean_data(url))\n",
    "\n",
    "# Save the cleaned data to a new Excel file\n",
    "df[\"Cleaned_Text\"] = cleaned_data\n",
    "output_file = \"cleaned_website_data.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "print(f\"Cleaned data saved at: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"cleaned_website_data.xlsx\"  # Replace with your actual file path\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(raw_text):\n",
    "    try:\n",
    "        # Step 1: Remove HTML tags and scripts\n",
    "        soup = BeautifulSoup(raw_text, \"html.parser\")\n",
    "        cleaned_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        # Step 2: Normalize whitespace\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "        # Step 3: Remove special characters\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned_text)\n",
    "\n",
    "        # Step 4: Convert to lowercase\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "\n",
    "        # Step 5 (Optional): Tokenization (if needed, uncomment below)\n",
    "        # tokens = cleaned_text.split()\n",
    "\n",
    "        # Step 6 (Optional): Remove stopwords (if needed, uncomment below)\n",
    "        # from nltk.corpus import stopwords\n",
    "        # stop_words = set(stopwords.words('english'))\n",
    "        # tokens = [word for word in tokens if word not in stop_words]\n",
    "        # cleaned_text = \" \".join(tokens)\n",
    "\n",
    "        return cleaned_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error processing text: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to: processed_website_data.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C-Dac\\AppData\\Local\\Temp\\ipykernel_10632\\663602529.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(raw_text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the \"Cleaned_Text\" column\n",
    "processed_df = df[['Cleaned_Text']].copy()\n",
    "processed_df['Processed_Text'] = processed_df['Cleaned_Text'].apply(preprocess_text)\n",
    "\n",
    "# Save the processed data to a separate Excel file\n",
    "output_file = \"processed_website_data.xlsx\"\n",
    "processed_df.to_excel(output_file, index=False)\n",
    "print(f\"Processed data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load processed data\n",
    "file_path = \"processed_website_data.xlsx\"\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C-Dac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "df['Embeddings'] = df['Processed_Text'].apply(lambda text: model.encode(text).tolist())\n",
    "\n",
    "# Save embeddings\n",
    "df.to_pickle(\"processed_data_with_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "df = pd.read_pickle(\"processed_data_with_embeddings.pkl\")\n",
    "embeddings = np.array(df['Embeddings'].tolist(), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance metric\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"semantic_search_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate query embedding\n",
    "query = \"Latest cricket news\"\n",
    "query_embedding = model.encode(query).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Processed_Text\n",
      "21  todays cricket match  cricket update  cricket ...\n",
      "0   live cricket score schedule latest news stats ...\n",
      "15  sporting news india  nba  cricket  football  t...\n",
      "10  bbc sport  scores fixtures news  live sport bb...\n",
      "8   the athletic uk  sports news commentary result...\n"
     ]
    }
   ],
   "source": [
    "# Search the index\n",
    "D, I = index.search(query_embedding, k=5)  # Retrieve top 5 results\n",
    "results = df.iloc[I[0]]\n",
    "print(results[['Processed_Text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a Pre-Trained NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\C-Dac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbecf49ab39048158cc7b7853f43e175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C-Dac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\C-Dac\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6123799c851430f8414f139a5a7b100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b87d4e426b348dca20dfd454ece9afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6319bf01cf47a0b340f466af769af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.999526, 'index': 1, 'word': 'Sa', 'start': 0, 'end': 2}, {'entity': 'I-PER', 'score': 0.99944085, 'index': 2, 'word': '##chin', 'start': 2, 'end': 6}, {'entity': 'I-PER', 'score': 0.9989986, 'index': 3, 'word': 'Ten', 'start': 7, 'end': 10}, {'entity': 'I-PER', 'score': 0.9954986, 'index': 4, 'word': '##du', 'start': 10, 'end': 12}, {'entity': 'I-PER', 'score': 0.97261775, 'index': 5, 'word': '##lk', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.99505556, 'index': 6, 'word': '##ar', 'start': 14, 'end': 16}, {'entity': 'I-LOC', 'score': 0.99975616, 'index': 9, 'word': 'India', 'start': 28, 'end': 33}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Example text for testing\n",
    "sample_text = \"Sachin Tendulkar played for India and scored 100 centuries in international cricket.\"\n",
    "\n",
    "# Perform NER\n",
    "entities = ner_pipeline(sample_text)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER results saved to 'ner_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the processed data\n",
    "file_path = \"processed_website_data.xlsx\"  # Replace with your file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Apply NER to each row\n",
    "df['Entities'] = df['Processed_Text'].apply(lambda text: ner_pipeline(text))\n",
    "\n",
    "# Save the results to a new file\n",
    "df.to_excel(\"ner_results.xlsx\", index=False)\n",
    "print(\"NER results saved to 'ner_results.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to Check Output After User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the NER results file\n",
    "file_path = \"ner_results.xlsx\"  # Replace with your actual file path\n",
    "df = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Function to handle user queries\n",
    "def process_query(query):\n",
    "    # Search for rows containing the query\n",
    "    results = df[df['Processed_Text'].str.contains(query, case=False, na=False)]\n",
    "    \n",
    "    if results.empty:\n",
    "        print(\"\\nNo results found for your query.\")\n",
    "    else:\n",
    "        # Display matched text and entities\n",
    "        print(\"\\nMatched Results:\")\n",
    "        for idx, row in results.iterrows():\n",
    "            print(f\"Text: {row['Processed_Text']}\")\n",
    "            print(\"Entities:\")\n",
    "            for entity in eval(row['Entities']):  # Convert string to list\n",
    "                print(f\"  - Entity: {entity['word']}, Type: {entity['entity']}, Confidence: {entity['score']:.2f}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Interactive loop for user input\n",
    "while True:\n",
    "    user_query = input(\"\\nEnter your query (or type 'exit' to quit): \").strip()\n",
    "    if user_query.lower() == \"exit\":\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    process_query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
